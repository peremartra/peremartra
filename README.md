# Pere Martra | Research Engineer & LLM Optimization Expert
*Rearchitecting LLMs.*

[![Contact for Collaboration](https://img.shields.io/badge/Contact%20for-Collaboration-brightgreen)](mailto:peremartra@uadla.com)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Pere%20Martra-blue)](https://linkedin.com/in/pere-martra)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Profile-yellow)](https://huggingface.co/oopere)

---

### Key Contributions & Active Research
My work is focused on two main pillars: making LLMs more efficient and making them more fair.

---

#### üîß **LLM Efficiency & Optimization**
| Type | Description |
|---|---|
| üìñ **Engineering** | [**Tailoring LLM Architectures**](https://github.com/peremartra/Rearchitecting-LLMs): Code on advanced structured pruning and optimization. |
| üî¨ **Research** | [**Adaptive Attention Bypass (AAB)**](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_6b_Adaptive_Inference_Attention_Pruning.ipynb): Prototype of a dynamic inference system that adjusts attention layers based on input complexity. |
| üìÑ **Preprint** | [**Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2**](https://www.techrxiv.org/users/1001026/articles/1361522-fragile-knowledge-robust-instruction-following-the-width-pruning-dichotomy-in-llama-3-2): My published methodology for structured pruning. [**Repository**](https://github.com/peremartra/llama-glu-expansion-pruning) |
| ‚úçÔ∏è **Article** | [**How to Prune LLaMA 3.2 and Similar LLMs**](https://medium.com/data-science/how-to-prune-llama-3-2-and-similar-large-language-models-cf18e9a2afb6): A practical guide on *Towards Data Science*. |

---

#### ‚öñÔ∏è **Fairness-Aware AI**
| Type | Description |
|---|---|
| üî• **Flagship Library** | [**OptiPfair**](https://github.com/peremartra/optipfair): My open-source library for fairness-aware model pruning and bias visualization. **[Try the Interactive Demo on HF Spaces!](https://huggingface.co/spaces/oopere/optipfair-bias-analyzer)** |
| üî¨ **Research** | [**Fairness Pruning Notebook**](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/8_2_Targeted_Pruning_for_Bias_Mitigation.ipynb): Implementation of a methodology for targeted neuron pruning to mitigate demographic bias. |
| ‚úçÔ∏è **Article** | [**Fairness Pruning: Precision Surgery to Reduce Bias in LLMs**](https://towardsdatascience.com/fairness-pruning-precision-surgery-to-reduce-bias-in-llms/): My core methodology published on *Towards Data Science*. |

---

### Author & Educator

<table>
  <tr>
    <td width="140">
      <a href="https://link.springer.com/book/10.1007/979-8-8688-0515-8">
        <img src="https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Large_Language_Models_Projects_Book.jpg" alt="Book Cover for Large Language Models Projects" width="120">
      </a>
    </td>
    <td>
      I wrote <b><a href="https://link.springer.com/book/10.1007/979-8-8688-0515-8">"Large Language Models Projects"</a></b> (Apress, 2024) to provide practical, hands-on guidance for AI practitioners. 
      <br><br>
      The accompanying <a href="https://github.com/peremartra/Large-Language-Model-Notebooks-Course"><b>LLM Course on GitHub</b></a> is a community resource with over <b>1,700 stars</b>. 
    </td>
  </tr>
</table>

### Areas of Collaboration
I am currently focused on my research and writing. I am open to select, high-impact collaborations where I can provide strategic value through **Technical Advisory** or **Applied Research Partnerships**.

### Connect with Me
- üîó **LinkedIn**: [Pere Martra](https://linkedin.com/in/pere-martra)
- ü§ó **Hugging Face**: [oopere](https://huggingface.co/oopere)
- ‚úçÔ∏è **Medium**: [@peremartra](https://medium.com/@peremartra)
- üìß **Direct Inquiries**: `peremartra [at] uadla [dot] com`

---

![GitHub Stats](https://github-readme-stats.vercel.app/api?username=peremartra&show_icons=true&theme=radical)
